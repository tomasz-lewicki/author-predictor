{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "import keras\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras import layers\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from utils.dataset import get_sentences, get_corpora, get_top_author_gut_idx\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' # surpress tensorflow warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Acquire the dataset\n",
    "First time this process takes around 10 minutes with fast internet connection.\n",
    "Later the texts are taken from a local mysql cache and the process is much faster (~1m)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_PICKLED_CORPORA = True\n",
    "\n",
    "if USE_PICKLED_CORPORA:\n",
    "    with open('corpora.pkl', 'rb') as f:\n",
    "        corpora = pickle.load(f)\n",
    "else:\n",
    "    # seed ensures we obtain author corpus from the same books each time.\n",
    "    # If set to None, we will sample at random\n",
    "    corpora = get_corpora(max_chars_per_author = 1e7, random_seed=42) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize the input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB = 30000\n",
    "t = keras.preprocessing.text.Tokenizer(\n",
    "    num_words=VOCAB, filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n', lower=True,\n",
    "    split=' ', char_level=False, oov_token=None, document_count=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.fit_on_texts(list(corpora.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert corpora into sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpora_seq = {}\n",
    "SEQ_LEN = 100\n",
    "for author in corpora:\n",
    "    s = t.texts_to_sequences([corpora[author]])\n",
    "    corpus_seq = np.array(s).squeeze()\n",
    "    \n",
    "    # reject last corp_len%seq_len words\n",
    "    corp_len = corpus_seq.shape[0]\n",
    "    aligned_len = corp_len - corp_len%SEQ_LEN \n",
    "    sequences = corpus_seq[:aligned_len].reshape(-1,100)\n",
    "    \n",
    "    corpora_seq[author] = sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make vectors X, y\n",
    "X - array of shape (n_samples, seq_len)\n",
    "\n",
    "y - array of lables (n_samples, n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = [], []\n",
    "for author, sequence in corpora_seq.items():\n",
    "    X.append(sequence)\n",
    "    y.append(len(sequence)*[author])\n",
    "X = np.vstack(X)\n",
    "y = np.concatenate(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = sklearn.preprocessing.LabelEncoder()\n",
    "le.fit(y)\n",
    "y = le.transform(y)\n",
    "y = keras.utils.to_categorical(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, X_test, y, y_test = sklearn.model_selection.train_test_split(X, y, test_size=0.1, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1D CNN Neural Net Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(VOCAB, BATCH_SIZE, SEQ_LEN, N_CLASSSES, LR):\n",
    "    model = keras.Sequential()\n",
    "    model.add(layers.Embedding(VOCAB, BATCH_SIZE, input_length=SEQ_LEN))\n",
    "\n",
    "    model.add(layers.Conv1D(filters=250,\n",
    "                            kernel_size=3,\n",
    "                            padding='valid',\n",
    "                            activation='relu',\n",
    "                            strides=1))\n",
    "\n",
    "    # we use max pooling:\n",
    "    model.add(layers.GlobalMaxPooling1D())\n",
    "\n",
    "    # We add a vanilla hidden layer:\n",
    "    model.add(layers.Dense(250, activation='relu'))\n",
    "    # model.add(layers.Dropout(0.2))\n",
    "    model.add(layers.Dense(N_CLASSSES, activation='sigmoid'))\n",
    "    \n",
    "    optim = keras.optimizers.adam(lr=LR)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer=optim,\n",
    "                  metrics=['accuracy', keras.metrics.categorical_accuracy])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training on the entire train dataset\n",
    "(no k-fold cross-validation at this point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tomek/anaconda3/envs/cmpe255/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "341802/341802 [==============================] - 45s 132us/step - loss: 0.0588 - accuracy: 0.9814 - categorical_accuracy: 0.7756\n",
      "Epoch 2/20\n",
      "341802/341802 [==============================] - 45s 131us/step - loss: 0.0220 - accuracy: 0.9925 - categorical_accuracy: 0.9247\n",
      "Epoch 3/20\n",
      "341802/341802 [==============================] - 44s 130us/step - loss: 0.0124 - accuracy: 0.9957 - categorical_accuracy: 0.9603\n",
      "Epoch 4/20\n",
      "341802/341802 [==============================] - 45s 130us/step - loss: 0.0076 - accuracy: 0.9974 - categorical_accuracy: 0.9770\n",
      "Epoch 5/20\n",
      "341802/341802 [==============================] - 44s 129us/step - loss: 0.0022 - accuracy: 0.9993 - categorical_accuracy: 0.9945\n",
      "Epoch 6/20\n",
      "341802/341802 [==============================] - 45s 131us/step - loss: 6.7693e-04 - accuracy: 0.9998 - categorical_accuracy: 0.9989\n",
      "Epoch 7/20\n",
      "341802/341802 [==============================] - 44s 130us/step - loss: 2.0880e-04 - accuracy: 1.0000 - categorical_accuracy: 0.9998\n",
      "Epoch 8/20\n",
      "341802/341802 [==============================] - 45s 131us/step - loss: 5.8876e-05 - accuracy: 1.0000 - categorical_accuracy: 1.0000\n",
      "Epoch 9/20\n",
      "341802/341802 [==============================] - 44s 130us/step - loss: 1.4962e-05 - accuracy: 1.0000 - categorical_accuracy: 1.0000\n",
      "Epoch 10/20\n",
      "341802/341802 [==============================] - 45s 131us/step - loss: 3.7394e-06 - accuracy: 1.0000 - categorical_accuracy: 1.0000\n",
      "Epoch 11/20\n",
      "341802/341802 [==============================] - 44s 129us/step - loss: 1.1345e-06 - accuracy: 1.0000 - categorical_accuracy: 1.0000\n",
      "Epoch 12/20\n",
      "341802/341802 [==============================] - 44s 129us/step - loss: 3.2764e-07 - accuracy: 1.0000 - categorical_accuracy: 1.0000\n",
      "Epoch 13/20\n",
      "341802/341802 [==============================] - 44s 129us/step - loss: 1.3454e-07 - accuracy: 1.0000 - categorical_accuracy: 1.0000\n",
      "Epoch 14/20\n",
      "341802/341802 [==============================] - 44s 129us/step - loss: 7.2149e-08 - accuracy: 1.0000 - categorical_accuracy: 1.0000\n",
      "Epoch 15/20\n",
      "341802/341802 [==============================] - 44s 129us/step - loss: 4.7849e-08 - accuracy: 1.0000 - categorical_accuracy: 1.0000\n",
      "Epoch 16/20\n",
      "341802/341802 [==============================] - 44s 129us/step - loss: 3.5406e-08 - accuracy: 1.0000 - categorical_accuracy: 1.0000\n",
      "Epoch 17/20\n",
      "341802/341802 [==============================] - 44s 129us/step - loss: 2.7973e-08 - accuracy: 1.0000 - categorical_accuracy: 1.0000\n",
      "Epoch 18/20\n",
      "341802/341802 [==============================] - 44s 129us/step - loss: 2.3093e-08 - accuracy: 1.0000 - categorical_accuracy: 1.0000\n",
      "Epoch 19/20\n",
      "341802/341802 [==============================] - 44s 129us/step - loss: 1.9604e-08 - accuracy: 1.0000 - categorical_accuracy: 1.0000\n",
      "Epoch 20/20\n",
      "341802/341802 [==============================] - 44s 130us/step - loss: 1.7019e-08 - accuracy: 1.0000 - categorical_accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7f47cfd07150>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N_CLASSSES = len(np.unique(np.argmax(y, axis=1)))\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 20\n",
    "LR = 1.62e-3 # chosen in k-fold cross-validation\n",
    "\n",
    "model = make_model(VOCAB, BATCH_SIZE, SEQ_LEN, N_CLASSSES, LR)\n",
    "\n",
    "lr_sched = keras.callbacks.LearningRateScheduler(lambda epoch, lr: LR if epoch <= 3 else LR/10)\n",
    "model.fit(X, y,\n",
    "          batch_size=BATCH_SIZE,\n",
    "          epochs=EPOCHS,\n",
    "          validation_data=None,\n",
    "          callbacks=[lr_sched])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9200853125493706"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = model.predict(X_test, batch_size=64)\n",
    "pred = np.argmax(scores, axis=1)\n",
    "truth = np.argmax(y_test, axis=1)\n",
    "\n",
    "# predictions are batch-aligned\n",
    "pred = pred[:len(truth)]\n",
    "(pred == truth).sum()/len(truth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🥳"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save necesary components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('label_encoder.pkl', 'wb') as f:\n",
    "    pickle.dump(le, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('corpora.pkl', 'wb') as f:\n",
    "    pickle.dump(corpora, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tokenizer.json', 'w') as f:\n",
    "    f.write(t.to_json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference examples in the next notebook!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "255-data-mining",
   "language": "python",
   "name": "255-data-mining"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
